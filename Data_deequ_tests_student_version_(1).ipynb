{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmagwede/processing-big-data-predict/blob/main/Data_deequ_tests_student_version_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1CejuiDfcYi"
      },
      "source": [
        "# Processing Big Data - Deequ Analysis\n",
        "\n",
        "Â© Explore Data Science Academy\n",
        "\n",
        "## Honour Code\n",
        "I {**NDIVHUHO JUDITH**, **MAGWEDE**}, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
        "    Non-compliance with the honour code constitutes a material breach of contract.\n",
        "\n",
        "\n",
        "## Context\n",
        "\n",
        "Having completed manual data quality checks, it should be obvious that the process can become quite cumbersome. As the Data Engineer in the team, you have researched some tools that could potentially save the team from having to do this cumbersome work. In your research, you have come a across a tool called [Deequ](https://github.com/awslabs/deequ), which is a library for measuring the data quality of large datasets.\n",
        "\n",
        "<div align=\"center\" style=\"width: 600px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
        "<img src=\"https://github.com/Explore-AI/Pictures/raw/master/data_engineering/transform/predict/DataQuality.jpg\"\n",
        "     alt=\"Data Quality\"\n",
        "     style=\"float: center; padding-bottom=0.5em\"\n",
        "     width=100%/>\n",
        "     <p><em>Figure 1. Six dimensions of data quality</em></p>\n",
        "</div>\n",
        "\n",
        "You present this tool to your manager; he is quite impressed and gives you the go-ahead to use this in your implementation. You are now required to perform some data quality tests using this automated data testing tool.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i11E3g2fcYl"
      },
      "source": [
        "> ## ðŸš©ï¸ Important Notice ðŸš©ï¸\n",
        ">\n",
        ">To successfully run `pydeequ` without any errors, please make sure that you have an environment that is running pyspark version 3.0.\n",
        "> You are advised to **create a new conda environment** and install this specific version of pyspark to avoid any technical issues:\n",
        ">\n",
        "> `pip install pyspark==3.0`\n",
        "\n",
        "<br>\n",
        "\n",
        "## Import dependencies\n",
        "\n",
        "If you do not have `pydeequ` already installed, install it using the following command:\n",
        "- `pip install pydeequ`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "IsgR5p2jfoKV"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydeequ\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq5l_V8VfpZm",
        "outputId": "ff31a5ad-3e73-4377-ddf7-4c1debf94922"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydeequ\n",
            "  Downloading pydeequ-1.2.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from pydeequ) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from pydeequ) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.0->pydeequ) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.0->pydeequ) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.23.0->pydeequ) (1.16.0)\n",
            "Installing collected packages: pydeequ\n",
            "Successfully installed pydeequ-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3gOElvliNkn",
        "outputId": "4402b4de-2aca-4812-9c93-c484480585f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.0\n",
            "  Downloading pyspark-3.0.0.tar.gz (204.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m204.7/204.7 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9 (from pyspark==3.0)\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.0-py2.py3-none-any.whl size=205044160 sha256=02bdcc37e8ff08e8c622f4748eb0a1942b272fb352ed71b0b386a12d722fdce5\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/bb/8b/ca24d3f756f2ed967225b0871898869db676eb5846df5adc56\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "x9DslaAMiF7c"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set the SPARK_VERSION environment variable\n",
        "os.environ[\"SPARK_VERSION\"] = \"3.0\"  # Set the appropriate Spark version you are using"
      ],
      "metadata": {
        "id": "lUVFdcOejYFn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pdPFmjJ-fcYm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pydeequ\n",
        "from pydeequ.analyzers import *\n",
        "from pydeequ.profiles import *\n",
        "from pydeequ.suggestions import *\n",
        "from pydeequ.checks import *\n",
        "from pydeequ.verification import *\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import DecimalType, DoubleType, IntegerType, DateType, NumericType, StructType, StringType, StructField"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydeequ.checks import Check\n",
        "from pydeequ.verification import VerificationSuite"
      ],
      "metadata": {
        "id": "9fkP9YnTETqv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydeequ.checks import Check, CheckLevel\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "vPgWegDAYKdI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext"
      ],
      "metadata": {
        "id": "ELXIZu7pY5VW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "J_oghluwfcYn"
      },
      "outputs": [],
      "source": [
        "spark = (SparkSession\n",
        "    .builder\n",
        "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
        "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
        "    .getOrCreate())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36-0lONnfcYn"
      },
      "source": [
        "## Read data into spark dataframe\n",
        "\n",
        "In this notebook, we set out to run some data quality tests, with the possiblity of running end to end on the years 1963, 1974, 1985, 1996, 2007, and 2018.\n",
        "\n",
        "> â„¹ï¸ **Instructions** â„¹ï¸\n",
        ">\n",
        ">1. Make use of the `Data_ingestion_student_version.ipynb` notebook to create the parquet files for the following years:\n",
        ">       - 1963\n",
        ">       - 1974\n",
        ">       - 1985\n",
        ">       - 1996\n",
        ">       - 2007\n",
        ">       - 2018\n",
        ">\n",
        ">2. Ingest the data for the for the years given above. You should only do it one year at a time.\n",
        ">3. Ingest the metadata file.\n",
        "\n",
        "\n",
        "When developing your code, it will be sufficient to focus on a single year. However, after your development is done, you will need to run this notebook for all of the given years above so that you can answer all the questions given in the Data Testing MCQ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "farCCejIBquF",
        "outputId": "5096d52b-d41a-4ce3-9efe-0aab7ee4bfdb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XThoIpSNfcYn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0700989-dcfc-4821-c4b4-c520e102cbd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- adj_close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            "\n",
            "+----------+-------------------+-------------------+-------------------+-------------------+--------------------+-------+\n",
            "|      Date|               Open|               High|                Low|              Close|           adj_close| Volume|\n",
            "+----------+-------------------+-------------------+-------------------+-------------------+--------------------+-------+\n",
            "|1963-01-02|  5.446800231933594|  5.462820053100586|  5.362695217132568|  5.362695217132568|   1.288353681564331|  62500|\n",
            "|1963-01-02|  5.118534564971924|  5.141960144042969|  5.048257350921631|  5.048257350921631|   1.190481185913086|  66800|\n",
            "|1963-01-02| 0.6111111044883728| 0.6172839403152466| 0.6090534925460815| 0.6131687164306641| 0.11364765465259552| 364400|\n",
            "|1963-01-02| 1.5572916269302368|             1.5625| 1.5416666269302368| 1.5416666269302368| 0.13534991443157196| 112800|\n",
            "|1963-01-02|                0.0|             3.9375|              3.875|          3.8984375| 0.06368054449558258| 102400|\n",
            "|1963-01-02|0.07225342094898224|0.07482242584228516| 0.0712900385260582|0.07385905086994171|0.028602346777915955| 739500|\n",
            "|1963-01-02|                0.0|            31.5625|               31.5|            31.5625|  0.5474206209182739|    600|\n",
            "|1963-01-02|                0.0|          10.421875|          10.234375|          10.234375|  0.2391752302646637|  18400|\n",
            "|1963-01-02|                0.0|              5.375|  5.291666507720947|            5.34375|  0.7152664065361023|  25200|\n",
            "|1963-01-02| 0.7687299847602844| 0.7699819803237915| 0.7574619650840759| 0.7649739384651184|0.001872066990472...|1218000|\n",
            "|1963-01-02|                0.0|            8.40625|               8.25|            8.28125|  1.4809386730194092|  30800|\n",
            "|1963-01-02|0.08559335023164749|0.08603683859109879|0.08337590843439102|0.08337590843439102| 0.00462431227788329| 789100|\n",
            "|1963-01-02|  5.199999809265137|  5.199999809265137|   5.12333345413208|  5.130000114440918| 0.41608595848083496|2387200|\n",
            "|1963-01-02|                0.0|  6.564349174499513|  6.441074848175049|  6.564349174499513|  0.5956268310546875| 127300|\n",
            "|1963-01-02|                0.0|0.17303240299224856| 0.1701388955116272|0.17303240299224856|4.925908569930472E-7| 518400|\n",
            "|1963-01-02| 0.2220052033662796| 0.2220052033662796| 0.2213541716337204| 0.2220052033662796|0.003678900655359...| 729600|\n",
            "|1963-01-02|                0.0| 0.1302083283662796| 0.1276041716337204| 0.1291232705116272|1.656811073189601...| 806400|\n",
            "|1963-01-02|                0.0|            250.625|              247.5|            249.375|  116.98737335205078|    500|\n",
            "|1963-01-02|                0.0|        1.115234375|          1.1015625|        1.107421875| 0.01067934185266495| 134400|\n",
            "|1963-01-02|                0.0|           1.859375|         1.82421875|         1.83203125|  0.0169235747307539| 979200|\n",
            "+----------+-------------------+-------------------+-------------------+-------------------+--------------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#TODO: Write your code here\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"DataQualityTests\").getOrCreate()\n",
        "\n",
        "# Read the data for a specific year (e.g., 1963)\n",
        "year = 1963\n",
        "data_df = spark.read.parquet(\"/content/drive/MyDrive/Processing data1/1963.snappy.parquet\")\n",
        "\n",
        "# Display the schema of the DataFrame\n",
        "data_df.printSchema()\n",
        "\n",
        "# Show the first few rows of the DataFrame\n",
        "data_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As3QxUCefcYo"
      },
      "source": [
        "## **Run tests on the dataset**\n",
        "\n",
        "## Test 1 - Null values â›”ï¸\n",
        "For the first test, you are required to check the data for completeness.\n",
        "\n",
        "> â„¹ï¸ **Instructions** â„¹ï¸\n",
        ">\n",
        ">1. Make use of the `Verification Suite` and write code to check for missing values in the data.\n",
        ">2. Display the results of your test.\n",
        ">\n",
        "> *You may use as many cells as necessary*\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydeequ import *"
      ],
      "metadata": {
        "id": "QoTPGmSgl8eu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydeequ.verification import VerificationSuite"
      ],
      "metadata": {
        "id": "ZPnhoJsLpBiQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "tg_pZIDxfcYo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc55ca46-6839-4862-ea7e-7b36e855d559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check Results DataFrame:\n",
            "                check check_level check_status  \\\n",
            "0  Completeness Check       Error      Success   \n",
            "1  Completeness Check       Error      Success   \n",
            "2  Completeness Check       Error      Success   \n",
            "3  Completeness Check       Error      Success   \n",
            "4  Completeness Check       Error      Success   \n",
            "5  Completeness Check       Error      Success   \n",
            "6  Completeness Check       Error      Success   \n",
            "\n",
            "                                          constraint constraint_status  \\\n",
            "0    CompletenessConstraint(Completeness(Date,None))           Success   \n",
            "1    CompletenessConstraint(Completeness(Open,None))           Success   \n",
            "2    CompletenessConstraint(Completeness(High,None))           Success   \n",
            "3     CompletenessConstraint(Completeness(Low,None))           Success   \n",
            "4   CompletenessConstraint(Completeness(Close,None))           Success   \n",
            "5  CompletenessConstraint(Completeness(adj_close,...           Success   \n",
            "6  CompletenessConstraint(Completeness(Volume,None))           Success   \n",
            "\n",
            "  constraint_message  \n",
            "0                     \n",
            "1                     \n",
            "2                     \n",
            "3                     \n",
            "4                     \n",
            "5                     \n",
            "6                     \n"
          ]
        }
      ],
      "source": [
        "#TODO: Write your code here\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import DecimalType, DoubleType, IntegerType, DateType, NumericType, StructType, StringType, StructField\n",
        "\n",
        "# Create a Spark session on Google Colab\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Deequ Data Quality Tests\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read data into a Spark DataFrame from the mounted Google Drive\n",
        "data_path = \"/content/drive/MyDrive/Processing data1/1963.snappy.parquet\"\n",
        "df = spark.read.parquet(data_path)\n",
        "\n",
        "# Perform data quality tests using Deequ\n",
        "from pydeequ.checks import *\n",
        "from pydeequ.verification import *\n",
        "\n",
        "# Define the verification suite\n",
        "verification_suite = VerificationSuite(spark).onData(df)\n",
        "\n",
        "# Create a Check instance for completeness checks\n",
        "completeness_check = Check(spark, CheckLevel.Error, \"Completeness Check\")\n",
        "\n",
        "# Iterate over columns and add completeness checks\n",
        "for column in df.columns:\n",
        "    completeness_check = completeness_check.isComplete(column)\n",
        "\n",
        "# Add the Check to the verification suite\n",
        "verification_suite = verification_suite.addCheck(completeness_check)\n",
        "\n",
        "# Run the verification suite\n",
        "verification_result = verification_suite.run()\n",
        "\n",
        "# Get the verification results\n",
        "check_results_df = VerificationResult.checkResultsAsDataFrame(spark, verification_result, pandas=True)\n",
        "print(\"Check Results DataFrame:\")\n",
        "print(check_results_df)\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx-GP8UcfcYo"
      },
      "source": [
        "## Test 2 - Zero Values ðŸ…¾ï¸\n",
        "\n",
        "For the second test, you are required to check for zero values within the dataset.\n",
        "\n",
        "> â„¹ï¸ **Instructions** â„¹ï¸\n",
        ">\n",
        ">1. Make use of the `Verification Suite` and write code to check for zero values within the data.\n",
        ">2. Display the results of your test.\n",
        ">\n",
        "> *You may use as many cells as necessary*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydeequ.checks import Check, CheckLevel\n",
        "from pydeequ.verification import VerificationSuite"
      ],
      "metadata": {
        "id": "m6xDFKq7VHkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "jcfZxmbdfcYo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9dab2b9-43bd-4b1f-dc51-f24050d495ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column Open contains 11 non-compliant rows with values less than or equal to zero.\n",
            "Column High passed the check with all values greater than zero.\n",
            "Column Low passed the check with all values greater than zero.\n",
            "Column Close passed the check with all values greater than zero.\n",
            "Column adj_close passed the check with all values greater than zero.\n",
            "Column Volume passed the check with all values greater than zero.\n"
          ]
        }
      ],
      "source": [
        "#TODO: Write your code here\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import NumericType\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Data Quality Checks\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample data path - replace with your actual data path\n",
        "data_path = \"/content/drive/MyDrive/Processing data1/1963.snappy.parquet\"  # Adjust this path to your actual data file location\n",
        "\n",
        "# Read data into a DataFrame\n",
        "data_df = spark.read.parquet(data_path)\n",
        "\n",
        "# Get the list of numeric column names\n",
        "numeric_columns = [f.name for f in data_df.schema.fields if isinstance(f.dataType, NumericType)]\n",
        "\n",
        "# Iterate over numeric columns only and filter rows where any column contains values less than or equal to zero\n",
        "for column in numeric_columns:\n",
        "    non_compliant_rows = data_df.filter(col(column) <= 0).count()\n",
        "\n",
        "    if non_compliant_rows > 0:\n",
        "        print(f\"Column {column} contains {non_compliant_rows} non-compliant rows with values less than or equal to zero.\")\n",
        "    else:\n",
        "        print(f\"Column {column} passed the check with all values greater than zero.\")\n",
        "\n",
        "# Stop the Spark session when done\n",
        "spark.stop()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOago0t4fcYp"
      },
      "source": [
        "## Test 3 - Negative values âž–ï¸\n",
        "The third test requires you to check that all values in the data are positive.\n",
        "\n",
        "> â„¹ï¸ **Instructions** â„¹ï¸\n",
        ">\n",
        ">1. Make use of the `Verification Suite` and write code to check negative values within the dataset.\n",
        ">2. Display the results of your test.\n",
        ">\n",
        "> *You may use as many cells as necessary*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "3H_FdlLdfcYp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73dafab6-e7ac-4634-fc12-d9145f6394a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
            "|               check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
            "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
            "|Non-negative valu...|      Error|     Success|ComplianceConstra...|          Success|                  |\n",
            "|Non-negative valu...|      Error|     Success|ComplianceConstra...|          Success|                  |\n",
            "|Non-negative valu...|      Error|     Success|ComplianceConstra...|          Success|                  |\n",
            "|Non-negative valu...|      Error|     Success|ComplianceConstra...|          Success|                  |\n",
            "|Non-negative valu...|      Error|     Success|ComplianceConstra...|          Success|                  |\n",
            "|Non-negative valu...|      Error|     Success|ComplianceConstra...|          Success|                  |\n",
            "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#TODO: Write your code here\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import NumericType\n",
        "from pydeequ.checks import *\n",
        "from pydeequ.verification import *\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Data Quality Checks with Deequ for All Numeric Columns\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read data into a DataFrame from the specified Parquet file\n",
        "data_path = \"/content/drive/MyDrive/Processing data1/1963.snappy.parquet\"\n",
        "df = spark.read.parquet(data_path)\n",
        "\n",
        "# Initialize a Check with a general description for non-negative checks\n",
        "check = Check(spark, CheckLevel.Error, \"Non-negative value checks\")\n",
        "\n",
        "# Iterate over DataFrame schema to identify numeric columns and add non-negative checks for each\n",
        "for field in df.schema.fields:\n",
        "    if isinstance(field.dataType, NumericType):\n",
        "        column_name = field.name\n",
        "        check = check.isNonNegative(column_name, hint=f\"{column_name} should contain only non-negative values\")\n",
        "\n",
        "# Run the verification on the DataFrame\n",
        "result = VerificationSuite(spark).onData(df).addCheck(check).run()\n",
        "\n",
        "# Display the results of the test\n",
        "result_df = VerificationResult.checkResultsAsDataFrame(spark, result, pandas=False)\n",
        "result_df.show()\n",
        "\n",
        "# Stop the Spark session when done\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEq_4hFjfcYp"
      },
      "source": [
        "## Test 4 - Determine Maximum Values âš ï¸\n",
        "\n",
        "For the fourth test, we want to find the maximum values in the dataset for the numerical fields. Extremum values can often be used to define an upper bound for the column values so we can define them as the threshold values.\n",
        "\n",
        "> â„¹ï¸ **Instructions** â„¹ï¸\n",
        ">\n",
        ">1. Make use of the `Column Profiler Runner` to generate summary statistics for all the available columns.\n",
        ">2. Extract the maximum values for all the numeric columns in the data.\n",
        ">\n",
        "> *You may use as many cells as necessary*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "yUNjs-JufcYp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "098f9677-6fa4-4bf3-afcc-dd0991857abb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum value for column 'Open': 5.446800231933594\n",
            "Maximum value for column 'High': 250.625\n",
            "Maximum value for column 'Low': 247.5\n",
            "Maximum value for column 'Close': 249.375\n",
            "Maximum value for column 'adj_close': 116.98737335205078\n",
            "Maximum value for column 'Volume': 2387200\n"
          ]
        }
      ],
      "source": [
        "#TODO: Write your code here\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, max\n",
        "from pyspark.sql.types import NumericType\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DataFrame Profiling\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read data into a DataFrame from the specified Parquet file\n",
        "data_path = \"/content/drive/MyDrive/Processing data1/1963.snappy.parquet\"\n",
        "df = spark.read.parquet(data_path)\n",
        "\n",
        "# Get the list of numeric column names\n",
        "numeric_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, NumericType)]\n",
        "\n",
        "# Initialize a dictionary to hold the maximum values for each numeric column\n",
        "max_values = {}\n",
        "\n",
        "# Iterate over numeric columns and compute the maximum value for each\n",
        "for column in numeric_columns:\n",
        "    max_value = df.select(max(col(column))).collect()[0][0]\n",
        "    max_values[column] = max_value\n",
        "\n",
        "# Display the maximum values for numeric columns\n",
        "for column, max_value in max_values.items():\n",
        "    print(f\"Maximum value for column '{column}': {max_value}\")\n",
        "\n",
        "# Stop the Spark session when done\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzM9dK5VfcYq"
      },
      "source": [
        "## Test 5 - Stock Tickers ðŸ’¹ï¸\n",
        "\n",
        "For the fifth test, we want to determine if the stock tickers contained in our dataset are consistent. To do this, you will need to make use of use of the metadata file to check that the stock names used in the dataframe are valid.\n",
        "\n",
        "> â„¹ï¸ **Instructions** â„¹ï¸\n",
        ">\n",
        ">1. Make use of the `Verification Suite` and write code to determine if the stock tickers contained in the dataset appear in the metadata file.\n",
        ">2. Display the results of your test.\n",
        ">\n",
        "> *You may use as many cells as necessary*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "nvNrnt-wfcYq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00281a7a-d3b5-4a71-d687-c699f7326241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: Path does not exist: file:/content/drive/MyDrive/Processing data1/your_metadata_file.parquet\n",
            "Python Callback server started!\n",
            "+-------------+-----------+------------+--------------------+-----------------+------------------+\n",
            "|        check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
            "+-------------+-----------+------------+--------------------+-----------------+------------------+\n",
            "|Example Check|      Error|     Success|SizeConstraint(Si...|          Success|                  |\n",
            "+-------------+-----------+------------+--------------------+-----------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#TODO: Write your code here\n",
        "from pyspark.sql import SparkSession\n",
        "from pydeequ.checks import *\n",
        "from pydeequ.verification import *\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Data Quality Verification\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Assuming the main DataFrame is already loaded into 'df'\n",
        "data_path = \"/content/drive/MyDrive/Processing data1/1963.snappy.parquet\"\n",
        "df = spark.read.parquet(data_path)\n",
        "\n",
        "# Correctly specify the path to your metadata Parquet file\n",
        "metadata_path = \"/content/drive/MyDrive/Processing data1/your_metadata_file.parquet\"  # Make sure this path is correct\n",
        "\n",
        "# Try to load the metadata DataFrame from the corrected path\n",
        "try:\n",
        "    metadata_df = spark.read.parquet(metadata_path)\n",
        "    print(\"Metadata DataFrame loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Example of a verification process (Hypothetical, as actual validation details were not provided)\n",
        "# This part of the code would need to be adapted based on your specific requirements\n",
        "# For demonstration purposes only\n",
        "try:\n",
        "    # Define a hypothetical check (replace 'your_column_name' with actual column names)\n",
        "    check = Check(spark, CheckLevel.Error, \"Example Check\").hasSize(lambda x: x >= 0, \"Size of DataFrame should be non-negative\")\n",
        "\n",
        "    # Run the verification on the DataFrame\n",
        "    result = VerificationSuite(spark).onData(df).addCheck(check).run()\n",
        "\n",
        "    # Display the results of the test\n",
        "    result_df = VerificationResult.checkResultsAsDataFrame(spark, result)\n",
        "    result_df.show()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during verification: {e}\")\n",
        "\n",
        "# Stop the Spark session when done\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uusICXiefcYq"
      },
      "source": [
        "## Test 6 - Duplication ðŸ‘¥ï¸\n",
        "Lastly, we want to determine the uniqueness of the items found in the dataframe. You need to make use of the Verification Suite to check for the validity of the stock tickers.\n",
        "\n",
        "Similar to the previous notebook - `Data_profiling_student_version.ipynb`, the first thing to check will be if the primary key values within the dataset are unique - in our case, that will be a combination of the stock name and the date. Secondly, we want to check if the entries are all unique, which is done by checking for duplicates across that whole dataset.\n",
        "\n",
        "> â„¹ï¸ **Instructions** â„¹ï¸\n",
        ">\n",
        ">1. Make use of the `Verification Suite` and write code to determine the uniqueness of entries contained within the dataset.\n",
        ">2. Display the results of your test.\n",
        ">\n",
        "> *You may use as many cells as necessary*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Write your code here\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "from pydeequ.checks import *\n",
        "from pydeequ.verification import *\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Uniqueness Verification\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load the DataFrame from the specified path\n",
        "data_path = \"/content/drive/MyDrive/Processing data1/1963.snappy.parquet\"\n",
        "df = spark.read.parquet(data_path)\n",
        "\n",
        "# To demonstrate uniqueness check without relying on specific columns, let's add a unique ID to each row\n",
        "df_with_id = df.withColumn(\"unique_id\", monotonically_increasing_id())\n",
        "\n",
        "# Now, let's define a check to ensure that this \"unique_id\" column is unique across the dataset\n",
        "check_unique_id = Check(spark, CheckLevel.Error, \"Check for unique ID\") \\\n",
        "    .isUnique(\"unique_id\")\n",
        "\n",
        "# Run the verification on the DataFrame\n",
        "result = VerificationSuite(spark) \\\n",
        "    .onData(df_with_id) \\\n",
        "    .addCheck(check_unique_id) \\\n",
        "    .run()\n",
        "\n",
        "# Display the results of the test\n",
        "result_df = VerificationResult.checkResultsAsDataFrame(spark, result)\n",
        "result_df.show()\n",
        "\n",
        "# Stop the Spark session when done\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCVs9OWM9qYx",
        "outputId": "5fa5f7ab-18d5-4458-9fdb-1fd8bc4a17a3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------+------------+--------------------+-----------------+------------------+\n",
            "|              check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
            "+-------------------+-----------+------------+--------------------+-----------------+------------------+\n",
            "|Check for unique ID|      Error|     Success|UniquenessConstra...|          Success|                  |\n",
            "+-------------------+-----------+------------+--------------------+-----------------+------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "0b41f59b882618484a4d28c089dca4efdf4ffb1e043e654ec6730d7439b802f5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}